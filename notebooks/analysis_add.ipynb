import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.preprocessing import MinMaxScaler

# Указываем путь к нашему кастомному импутеру
import sys
sys.path.append('../src')
from custom_imputer import KZImputer

# --- УНИВЕРСАЛЬНАЯ ФУНКЦИЯ ДЛЯ ПРОВЕДЕНИЯ ЭКСПЕРИМЕНТА ---

def run_imputation_experiment(data_path, target_column, date_column=None, date_format=None, **kwargs):
    """
    Проводит полный эксперимент по сравнению методов импутации на заданном датасете.
    
    Параметры:
        data_path (str): Путь к CSV файлу.
        target_column (str): Название колонки с временным рядом.
        date_column (str, optional): Название колонки с датой/временем.
        date_format (str, optional): Формат даты для pd.to_datetime.
        **kwargs: Дополнительные аргументы для pd.read_csv (например, sep=',').
    """
    print(f"--- Запуск эксперимента для датасета: {data_path.split('/')[-1]} ---")
    
    # 1. Загрузка и подготовка данных
    df = pd.read_csv(data_path, **kwargs)
    
    if date_column:
        df[date_column] = pd.to_datetime(df[date_column], format=date_format)
        df = df.set_index(date_column)

    # Выбираем целевую колонку
    series = df[target_column].astype(float)
    
    # Обработка уже существующих пропусков (для чистоты эксперимента)
    series = series.dropna()
    
    if len(series) < 100:
        print("Датасет слишком короткий, пропускаем.")
        return None

    # 2. Создание искусственных пропусков
    np.random.seed(42)
    test_series = series.copy()
    
    # Масштабируем количество пропусков к размеру датасета
    num_single_gaps = max(10, int(len(test_series) * 0.01))
    num_triple_gaps = max(3, int(len(test_series) * 0.005))
    num_penta_gaps = max(2, int(len(test_series) * 0.003))

    for _ in range(num_single_gaps):
        idx = np.random.randint(10, len(test_series) - 10)
        test_series.iloc[idx] = np.nan
    for _ in range(num_triple_gaps):
        idx = np.random.randint(10, len(test_series) - 15)
        test_series.iloc[idx:idx+3] = np.nan
    for _ in range(num_penta_gaps):
        idx = np.random.randint(10, len(test_series) - 20)
        test_series.iloc[idx:idx+5] = np.nan
    
    missing_indices = test_series.isna()
    print(f"Внесено {missing_indices.sum()} пропусков ({missing_indices.sum()/len(series):.2%}).")

    # 3. Сравнение методов импутации
    imputers = {
        "Kavun-Zamula": KavunZamulaImputer(max_gap_size=5),
        "Mean": SimpleImputer(strategy='mean'),
        "Median": SimpleImputer(strategy='median'),
        "Forward Fill": 'ffill',
        "Backward Fill": 'bfill',
        "Linear Interpolate": 'linear',
        "Spline Interpolate": 'spline',
        "KNN (k=5)": KNNImputer(n_neighbors=5),
        "IterativeImputer": IterativeImputer(max_iter=10, random_state=0)
    }
    
    results = {}
    test_series_2d = test_series.to_frame()

    for name, imputer in imputers.items():
        imputed_series = None
        try:
            if isinstance(imputer, str):
                if name.endswith("Interpolate"):
                    imputed_series = test_series.interpolate(method=name, order=3 if name == "Spline Interpolate" else None, limit_direction='both')
                else:
                    imputed_series = test_series.fillna(method=imputer)
            elif name == "Kavun-Zamula":
                imputed_series = imputer.transform(test_series)
            else:
                # Для KNN и Iterative, которые чувствительны к масштабу, применим Min-Max Scaling
                scaler = MinMaxScaler()
                scaled_data = scaler.fit_transform(test_series_2d)
                imputed_scaled = imputer.fit_transform(scaled_data)
                imputed_data = scaler.inverse_transform(imputed_scaled)
                imputed_series = pd.Series(imputed_data.flatten(), index=test_series.index)
            
            # Если после импутации остались пропуски (например, в начале/конце для ffill/bfill), заполним их медианой
            if imputed_series.isna().any():
                imputed_series = imputed_series.fillna(test_series.median())

            true_values = series[missing_indices]
            imputed_values = imputed_series[missing_indices]
            rmse = np.sqrt(mean_squared_error(true_values, imputed_values))
            results[name] = rmse
        except Exception as e:
            print(f"Ошибка при обработке '{name}': {e}")
            results[name] = np.nan
    
    return pd.Series(results, name=data_path.split('/')[-1])

# --- ЗАПУСК ЭКСПЕРИМЕНТОВ НА ВСЕХ ДАТАСЕТАХ ---

datasets_to_test = {
    'opsd_germany_daily.csv': {
        'target_column': 'Consumption', 
        'date_column': 'Date'
    },
    'PRSA_data_2010.1.1-2014.12.31.csv': {
        'target_column': 'pm2.5',
        'date_column': 'No', # Этот датасет не имеет колонки даты, будем использовать просто индекс
        'parser': lambda df: df.set_index(pd.to_datetime(df[['year', 'month', 'day', 'hour']]))
    },
    'DailyDelhiClimateTrain.csv': {
        'target_column': 'meantemp',
        'date_column': 'date'
    },
    'all_stocks_5yr.csv': {
        'target_column': 'close',
        'date_column': 'date',
        'filter_col': 'Name',
        'filter_val': 'AAL' # Выбираем акцию American Airlines
    },
    'AirPassengers.csv': {
        'target_column': '#Passengers',
        'date_column': 'Month'
    }
}

all_results = []

for filename, params in datasets_to_test.items():
    data_path = f'../data/{filename}'
    
    # Специальные обработчики для сложных случаев
    if 'parser' in params:
        df = pd.read_csv(data_path)
        df = params['parser'](df)
        df.to_csv(f'../data/processed_{filename}', index=True) # Сохраняем обработанный файл
        data_path = f'../data/processed_{filename}'
        params['date_column'] = df.index.name
    
    if 'filter_col' in params:
        df = pd.read_csv(data_path)
        df = df[df[params['filter_col']] == params['filter_val']]
        df.to_csv(f'../data/processed_{filename}', index=False)
        data_path = f'../data/processed_{filename}'

    result = run_imputation_experiment(
        data_path=data_path,
        target_column=params['target_column'],
        date_column=params['date_column']
    )
    if result is not None:
        all_results.append(result)

# Объединяем результаты в один DataFrame
final_results_df = pd.concat(all_results, axis=1)

# --- ВИЗУАЛИЗАЦИЯ СВОДНЫХ РЕЗУЛЬТАТОВ ---

# Нормализуем RMSE для каждого датасета (делим на RMSE худшего метода)
# Это позволит сравнивать их на одном графике
normalized_results = final_results_df.div(final_results_df.max(axis=0), axis=1)

plt.figure(figsize=(15, 8))
sns.heatmap(normalized_results.T, cmap='viridis_r', annot=final_results_df.T, fmt=".2f")
plt.title('Сравнение методов импутации (RMSE, чем меньше, тем лучше)')
plt.ylabel('Датасет')
plt.xlabel('Метод импутации')
plt.show()

# Ранжируем методы по среднему нормализованному RMSE
mean_ranks = normalized_results.mean(axis=1).sort_values()

plt.figure(figsize=(12, 6))
ax = sns.barplot(x=mean_ranks.index, y=mean_ranks.values)
ax.set_title('Средний нормализованный RMSE по всем датасетам')
ax.set_ylabel('Средний нормализованный RMSE')
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right")
plt.tight_layout()
plt.show()